{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":"<p>Note</p> <p>This workshop will give you hand-on experience with the AI Toolkit (AITK) in Visual Studio Code to prototype multimodal agents for your business scenario.</p>"},{"location":"#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this workshop, you should be able to: - Explore and compare models in the AITK Model catalog, to select the best fit for your use-case. - Augment models with prompts and data to get more accurate and grounded responses in the AITK Playground. - Prototype an agent by combining models and instructions with tools via MCP (Model Context Protocol) using the AITK Agent Builder.</p>"},{"location":"#lab-outline","title":"Lab Outline","text":"<p>The lab is organized into 4 sections, taking you through the process of prototyping a multimodal agent with the AI Toolkit for VS Code.</p> <ol> <li>Part 1 - Model Selection Model selection is an essential step in building AI solutions. In this section, you will explore the AI Toolkit Model Catalog to compare and select models that best fit your business scenario.</li> <li>Part 2 - Model Augmentation Once you have selected a model, you will learn how to augment it using prompt engineering and context data to improve its performance and relevance to your specific use case.</li> <li>Part 3 - Agent Prototyping In this section, you will use the AITK Agent Builder to prototype an agent. You will combine your selected and augmented models with instructions and tools via MCP (Model Context Protocol).</li> <li>Part 4 - From Prototype to Code Finally, you will learn how to export your agent prototype into code that can be integrated into your applications.</li> </ol> <p>Click Next to set up your Workshop environment and get started.</p>"},{"location":"01_Get_Started/","title":"Get started","text":"<p>Tip</p> <p>What is the AI Toolkit(AITK)? The AI Toolkit (AITK) is an extension for Visual Studio Code that provides a unified interface to access and interact with various AI models and services. It allows users to easily explore, compare, and utilize different AI models from multiple providers, both proprietary and open source, hosted on several platforms, such as Github, Microsoft Foundry or even locally. With AITK, developers can streamline their Generative AI development workflow by integrating model selection, prompt engineering, and agent prototyping and testing directly within their code editor.</p>"},{"location":"01_Get_Started/#open-workshop-in-a-github-codespace","title":"Open workshop in a GitHub Codespace","text":"<p>In this workshop, we will be using GitHub Codespaces to launch a cloud-hosted development environment with all the necessary tools and dependencies pre-installed. This will allow you to focus on learning and prototyping without worrying about local setup.</p> <ol> <li> <p>Open the browser and navigate to the GitHub repo hosting the lab assets.</p> <p>Tip</p> <p>Click the Star button in the top right corner, this will help you easily find it later.</p> </li> <li> <p>To launch a codespace, you need a GitHub account.</p> <p>Note</p> <p>If you already have a GitHub account, you can move to step 3 directly.</p> <p>To create one, click on the Sign up button and follow the instructions below:</p> <ul> <li>In the new window, enter a personal email address, create a password, and choose a username.</li> <li>Select your Country/Region and agree to the terms of service.</li> <li>Click on the Create account button and wait for the verification email to arrive in your inbox.</li> </ul> <p></p> <ul> <li>Copy the verification code from the email and paste it into the verification field on the GitHub website. Then click on Continue.</li> <li>Once the account is created, you'll be redirected back to the GitHub repo page and you'll see a green banner at the top, like the one in the screenshot below.</li> </ul> <p></p> </li> <li> <p>Click on Sign in and enter your GitHub credentials to log in. If you just created your account, use the username and password you set during the sign-up process.</p> </li> <li> <p>Next, click on the green Code button and select Create codespace on main from the dropdown menu.</p> <p></p> </li> <li> <p>Once the codespace is created, you'll see a Visual Studio Code environment loading in your browser. You might choose to continue working in the browser or click on the Open in VS Code button to open it in the desktop application (if you have it installed in your machine).</p> <p></p> </li> </ol> <p>Warning</p> <p>If you move to VS Code Desktop App, make sure the Github account you are logged in within VS Code is the same you used to create the GitHub Codespace.</p>"},{"location":"01_Get_Started/#verify-ai-toolkit-extension-is-installed","title":"Verify AI Toolkit extension is installed","text":"<p>In the GitHub Codespace, you should be able to see - among the Visual Studio Code extensions already installed - the AI Toolkit. This is the extension we will be using to interact with various AI models and services in this lab.</p> <p></p> <p>Tip</p> <p>If you don't see the extension icon, click on the ellipsis (...) at the bottom of the sidebar to see the full list of installed extensions.</p>"},{"location":"01_Get_Started/#ready-to-start","title":"Ready to start","text":"<p>That covers the necessary setup to work with the AI Toolkit in VScode and Github-hosted models. We will now move forward to begin exploring the Model Catalog and interacting with the models. Click Next to proceed to the following section of the lab.</p>"},{"location":"02_Model_Selection/","title":"Model Selection: Exploring the AI Toolkit Model Catalog","text":"<p>In this section, you will explore the AI Toolkit Model Catalog to discover, filter, and compare models for your multimodal agent project. The Model Catalog provides access to models from various providers including GitHub, Microsoft Foundry, OpenAI, and others.</p>"},{"location":"02_Model_Selection/#step-1-apply-filters-to-narrow-your-selection","title":"Step 1: Apply Filters to Narrow Your Selection","text":"<ol> <li>In your Codespace, locate the AI Toolkit extension icon in the left sidebar</li> <li>Click on the AI Toolkit icon to open the extension panel</li> <li>Click on Model Catalog to browse available models</li> </ol> <p>On the top of the page you'll find the most popular models; scroll down to see the full list of available models.</p> <p>Since the list is quite consistent, you can use the filtering options to narrow down the selection based on your requirements.</p>"},{"location":"02_Model_Selection/#filter-by-hosting-provider","title":"Filter by Hosting Provider","text":"<ol> <li>Click on the Hosted by filter dropdown. You have several options, such as GitHub, Microsoft Foundry, OpenAI and you can even leverage models hosted on your local infrastructure, through Ollama or ONNX.</li> <li>Select GitHub to view free-to-use models that are excellent for prototyping.</li> </ol> <p>Note</p> <p>GitHub models are perfect for getting started because they're free to use, but they are token-rate limited. You can experiment without cost concerns, but for production deployments consider using a pay-as-you-go offering through your GitHub Account or Microsoft Foundry.</p>"},{"location":"02_Model_Selection/#filter-by-model-features","title":"Filter by Model Features","text":"<ol> <li>Click on the Feature filter dropdown to filter by model capabilities, such as image/audio or video processing, tool calling, etc.</li> <li>Select Image Attachment to find multimodal models that support visual input processing and enables multimodal interactions combining text and images.</li> </ol>"},{"location":"02_Model_Selection/#filter-by-publisher","title":"Filter by Publisher","text":"<ol> <li>Click on the Publisher filter dropdown to filter by the model publisher, such as Microsoft, Meta, Cohere, etc. Note that you can find both open-source and proprietary models.</li> <li>Select OpenAI and Mistral AI to view models from these two leading providers.</li> </ol>"},{"location":"02_Model_Selection/#step-2-add-models-to-your-collection","title":"Step 2: Add Models to Your Collection","text":"<p>After applying filters, you'll see a refined list of models. For this exercise:</p> <ol> <li>Locate the GPT-4.1 and Mistral Small 3.1 models in the filtered results.</li> <li>GPT-4.1 is a full-featured multimodal model with comprehensive capabilities</li> <li>Mistral Small 3.1 is an optimized smaller model with faster response times and lower costs.</li> <li>Click Add model on each model tile to add them to your collection.</li> </ol> <p></p> <p>Note</p> <p>Once they are added, the blue button will change to green with the label Added.</p>"},{"location":"02_Model_Selection/#step-3-open-the-playground-for-testing","title":"Step 3: Open the Playground for Testing","text":"<ol> <li>Click on Model Playground in the AI Toolkit panel. The Playground allows you to test and compare models interactively.</li> <li>You'll be prompted to login to your GitHub account to access the free-tier models. Click Allow and complete the authentication process, by using the same GitHub credentials you used in the previous lab section.</li> </ol> <p>Tip</p> <p>Now that you are logged in, you should be able to see the models you added into your collection in the 'My resources' tab, under 'GitHub'. If you don't see them, click on the refresh icon to update the view.</p> <p></p> <ol> <li>In the Model field, select one of the two GitHub-hosted models you added to your collection, for example Mistral Small 3.1 (via GitHub). It will be loaded into the Playground automatically.</li> </ol> <p></p> <p>Note</p> <p>You might experience some delay in model loading, especially if it's your first time accessing the Playground. Please be patient while the model initializes.</p> <ol> <li>Next, click the Compare button to enable side-by-side comparison</li> <li>From the dropdown, select your second model (GPT-4.1 if Mistral Small 3.1 is already selected)</li> <li>You now have two models ready for comparison testing</li> </ol> <p></p>"},{"location":"02_Model_Selection/#step-4-test-text-generation-and-multimodal-capabilities","title":"Step 4: Test Text Generation and Multimodal Capabilities","text":"<p>Tip</p> <p>The side-by-side comparison allows you to see exactly how different models handle the same input, making it easier to choose the best fit for your specific use case.</p> <p>Let's start interacting with the models with a simple prompt:</p> <ol> <li>Enter this prompt in the text field (where you see the placeholder \"Type a prompt\"):    <pre><code>Create a short LinkedIn post about developer productivity with AI tools.\n</code></pre></li> <li>Click the paper airplane icon to execute the prompt on both models simultaneously</li> </ol> <p></p> <p>Now, test the models' image processing capabilities:</p> <ol> <li> <p>Enter this prompt in the text field:    <pre><code>Extract the text from the attached image.\n</code></pre></p> </li> <li> <p>Click the image attachment icon to add a picture as input</p> </li> </ol> <p></p> <ol> <li> <p>Select an image file to upload. You'll be prompted with a text field with a default file path in your workspace directory. Replace it with the following:    <pre><code>/workspaces/accelerate-ai-agents-development-with-gh-models-and-ai-toolkit/docs/img/gh_copilot_slide.png\n</code></pre> </p> </li> <li> <p>Send the multimodal prompt on both models simultaneously.</p> </li> </ol> <p>Observe how each model handles the image input and the accuracy of the extracted text.</p> <p>Next, let's test their reasoning capabilities, with the following prompt:</p> <pre><code>Create a content strategy plan for a social media management team currently focused on increasing awareness of AI-powered developer productivity tools. Include target audience and channels, key messages, content types, and a sample posting schedule for one week. Include rationale for your choices.\n</code></pre> <p>Look at how each model approaches the task, the depth of analysis, and the creativity in their strategies. Also, note the reasoning and justification provided in their responses.</p>"},{"location":"02_Model_Selection/#step-5-analyze-and-compare-results","title":"Step 5: Analyze and Compare Results","text":"<p>Review the outputs from both models, using several factors to guide your evaluation:</p> <ul> <li>Response Quality: Compare the depth and accuracy of descriptions, as well as the coherence with the input prompt.</li> <li>Detail Level: Which model provides more comprehensive analysis?</li> <li>Processing Time: Note any differences in response speed.</li> <li>Output Formatting: Evaluate clarity and organization of responses, as well as verbosity.</li> <li>Token Usage: Inspect the token usage for each model to understand cost implications. Note that token usage may vary not only based on the verbosity of the response but also on the tokenizer efficiency of each model.</li> </ul> <p>Tip</p> <p>Number of output tokens is visible in the response footer, along with characters length.</p> <p></p>"},{"location":"02_Model_Selection/#leverage-github-copilot-for-comparative-analysis","title":"Leverage GitHub Copilot for Comparative Analysis","text":"<p>To assist with the comparative analysis, you can leverage GitHub Copilot to generate a comparison summary.</p> <p>To access GitHub Copilot Chat, select the Toggle Chat icon at the top of the Visual Studio Code window.</p> <p></p> <p>Note</p> <p>If asked to log in at your first interaction with Copilot, select Sign-in -&gt; Continue with GitHub. Then click on Continue to proceed with the GitHub account you used to access the GitHub hosted models, when redirected to the GitHub sign-in page.</p> <p>Try the following prompt in the Copilot chat window:</p> <pre><code>I am exploring models for an AI agent that should support a social media management team creating content targeted to a developer audience, on different channels and formats. I am evaluating Mistral Small 3.1 and OpenAI GPT 4.1. Which one would you recommend for this scenario, and why? Explain the trade-offs between models (e.g., reasoning ability, cost, latency, context length) so that I can make an informed choice.\n</code></pre> <p>To answer this, Copilot calls the Get AI Model Guidance tool of the AI Toolkit, which provides model recommendations based on your use case. In the response, you should see an expandable section with the details of the tool call, followed by the comparative analysis.</p> <p></p> <p>Note</p> <p>If GitHub Copilot doesn't invoke the AI Toolkit tools when generating its response, you can enter <code>#aitk</code> in the chat window to explicitly select which tool(s) you'd like GitHub Copilot to use prior to submitting your prompt.</p>"},{"location":"02_Model_Selection/#step-6-select-a-model-for-next-steps","title":"Step 6: Select a Model for Next Steps","text":"<p>Once we are done with the comparison, we are going to select one of the two models for further prototyping in the next lab sections. For the sake of this exercise, let's go with GPT-4.1.  Click on Select this model on the right side of the model name.</p> <p></p>"},{"location":"02_Model_Selection/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>The Model Catalog provides a comprehensive view of available AI models from multiple providers</li> <li>Filtering capabilities help you quickly identify models that match your specific requirements</li> <li>Model comparison in the Playground enables data-driven decision making</li> <li>Different hosting options offer varying benefits for different stages of development</li> <li>Multimodal capabilities can be tested effectively using the built-in comparison tools</li> </ul> <p>This exploration process ensures you select the most appropriate model for your specific use case, balancing factors like performance, cost, features, and deployment requirements. Click Next to proceed to the following section of the lab.</p>"},{"location":"03_Model_Augmentation/","title":"Model Augmentation: Enhancing Context for Improved Performance","text":"<p>In this section, you will learn how to augment your selected model using prompt engineering and context data to improve its performance and relevance to your specific use case. This is a crucial step in tailoring AI models to meet the unique needs of your business scenario.</p>"},{"location":"03_Model_Augmentation/#step-1-crafting-the-system-message","title":"Step 1: Crafting the System Message","text":"<p>The system message is a critical component of the prompt that sets the behavior and context for the AI model. It helps the model understand its role and the specific requirements of the task at hand. Here are some key considerations for crafting an effective system message:</p> <ol> <li> <p>Be Clear and Concise: Clearly articulate the purpose of the interaction and the desired outcome. Avoid ambiguity to ensure the model understands the task.</p> </li> <li> <p>Provide Context: Include relevant background information or context that will help the model generate more accurate and context-aware responses.</p> </li> <li> <p>Set Expectations: Specify any constraints or requirements for the response, such as format, length, or style.</p> </li> <li> <p>Break Down Complex Instructions: If the task is complex, consider breaking it down into simpler, step-by-step instructions to guide the model effectively.</p> </li> </ol> <p>In the System Prompt field of the Playground, in the right pane, enter the following system message:</p> <pre><code>You are an intelligent and friendly AI assistant that supports a social media management team creating content targeted to a developer audience, on different channels and formats.\n\n# Task\nYour role is to:\n- Engage with users in natural conversation to understand their social media content creation goals.\n- Ask thoughtful questions to gather relevant project details.\n- Be brief in your responses.\n\nContent Formats:\n- Blog: Informative or narrative articles, structured with an introduction, body, and conclusion.\n- Video Script: Text for videos, including directions for scenes, dialogues, and actions.\n- Post Captions: Short and catchy texts to accompany images or videos on social media.\n\n# Personality\nYour personality is:\n- Warm and welcoming, like a helpful colleague.\n- Professional and knowledgeable, like a seasoned social media expert.\n- Curious and conversational\u2014never assume, always clarify.\n\n# Guardrails\n- Stick to the scenario above. If something falls outside social media content creation, respond to the user politely with your scope limits.\n</code></pre> <p>Note</p> <p>The model preference panel also includes some model parameters you can adjust to further refine the model's behavior, such as the max length of the response - Max Response Tokens - and the degree of randomness in the output - Temperature and Top P. For this exercise, you can leave them to their default values.</p> <p></p> <p>Note that this message includes:</p> <ul> <li>A clear definition of the assistant's role and responsibilities (\"You are an AI assistant that supports a social media management team...\")</li> <li>Specific instructions on how to interact with users and what to focus on (e.g., \"Engage with the users in natural conversation...\")</li> <li>Guidelines on tone and style to ensure consistent and appropriate responses (e.g., \"Warm and welcoming, like a helpful colleague...\")</li> <li>Safety guardrails to keep the assistant focused on relevant topics (\"Stick to the scenario above...\")</li> </ul>"},{"location":"03_Model_Augmentation/#step-2-testing-the-system-message-with-multimodal-input","title":"Step 2: Testing the System Message with Multimodal Input","text":"<p>Now that we configured the system prompt, let's test the system with a user prompt. Attach again the same image of the github copilot slide we used in the previous section, and combine it with the following user prompt:</p> <pre><code>Create a short LinkedIn post about developer productivity with the tool illustrated in the attached image.\n</code></pre> <p>The model will analyze the image and provides a suggestion for a LinkedIn post that aligns with the user's request and the guidelines set in the system message. Note how there's no reference to GitHub Copilot in the system message or the user prompt, yet the model is able to identify the tool in the image and incorporate it into the response.</p> <p>Let's now test the model with a user query which is not relevant to the given scenario. Enter the following prompt:</p> <pre><code>What\u2019s the weather like in Lecce today? \n</code></pre> <p>The model should politely inform the user that it can only assist with social media content creation, demonstrating its ability to follow the guidelines set in the system message.</p>"},{"location":"03_Model_Augmentation/#step-3-adding-grounding-data","title":"Step 3: Adding Grounding Data","text":"<p>In addition to the system message, providing context data can significantly enhance the model's ability to generate relevant and accurate responses. Context data can include information about your business, products, services, or any other relevant details that can help the model better understand the scenario.</p> <p>For our use case, we are going to provide the model with some context about the Global AI Community, the largest worldwide developer community focused on AI, with a special focus on the local chapters based in Italy.</p> <p>To add grounding data, we will use the file attachment feature in the Playground. This allows us to upload documents that the model can reference when generating responses.</p> <p>The document we are going to upload is a .docx file, containing information about the Global AI Community, including its mission, some key statistics and its presence in Italy. You can find the file in the <code>data</code> folder of your AITK environment, named <code>global_ai_community.docx</code>.</p> <ol> <li>Click the file attachment icon in the prompt input area. </li> <li>Select the file <code>global_ai_community.docx</code> from the <code>/workspaces/accelerate-ai-agents-development-with-gh-models-and-ai-toolkit/data</code> directory.</li> </ol> <p>Tip</p> <p>In the text field that appears, you can enter the following path to the file:</p> <pre><code>/workspaces/accelerate-ai-agents-development-with-gh-models-and-ai-toolkit/data/global_ai_community.docx\n</code></pre> <p></p> <ol> <li>Once the file is uploaded, it will appear as an attachment below the prompt input area.</li> <li>Enter the following prompt in the text field: <pre><code>Create a short LinkedIn post to promote the Global AI Community and its activities.\n</code></pre></li> </ol> <p>The model will analyze the uploaded document and provide a grounded suggestion for a LinkedIn post that highlights the Global AI Community's mission, key statistics, and its presence in Italy.</p> <p>What happens behind the scenes is that the attached data are automatically included in the prompt context, enabling the model to generate more informed and relevant responses.</p> <p>Of course this approach has its limitations, as the model can only process a limited amount of text in the prompt context, and the larger is the attached context the higher is the response latency and cost. For larger datasets or more complex scenarios, you need to implement a more sophisticated retrieval mechanism to ensure the model prompt includes only the most relevant information for the current user query. We are going to explore this in more detail in the next section of this workshop.</p>"},{"location":"03_Model_Augmentation/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Crafting an effective system message is crucial for guiding the model's behavior and ensuring relevant responses.</li> <li>Providing context data through file attachments can significantly enhance the model's performance and relevance.</li> <li>Testing the model with multimodal input helps validate the effectiveness of the system message and context data.</li> <li>Grounding data should be relevant and concise to fit within the model's input limitations.</li> </ul> <p>Click Next to proceed to the following section of the lab.</p>"},{"location":"04_Agent_Building/","title":"Agent Building: Building an AI Agent with Agent Builder","text":"<p>In this section, you will learn how to create an AI agent with Agent Builder in the AI Toolkit and equip it with tools, enabling it to take actions on behalf of the user. Agent Builder streamlines the engineering workflow for building agents, including prompt engineering and integration with tools, such as MCP servers.</p> <p>Tip</p> <p>Model Context Protocol (MCP) is a powerful, standardized framework that optimizes communication between Large Language Models (LLMs) and external tools, applications, and data sources.</p>"},{"location":"04_Agent_Building/#step-1-explore-agent-builder","title":"Step 1: Explore Agent Builder","text":"<p>To access Agent Builder, in the AI Toolkit view, select Agent Builder.</p> <p></p> <p>Agent Builder's UI is organized into two sections. The left side of Agent Builder enables you to define the basic information for the agent such as its name, model choice, instructions, and any relevant tools. The right side of Agent Builder is where you can both chat with the agent and evaluate the agent's responses.</p>"},{"location":"04_Agent_Building/#step-2-create-the-agent","title":"Step 2: Create the Agent","text":"<p>In Agent Builder configure the new agent's basic information. Within the Agent name field, enter Social Media Manager. For the agent's Model, select the OpenAI gpt-4.1 (via GitHub) model.</p> <p></p>"},{"location":"04_Agent_Building/#step-3-provide-instructions-for-the-agent","title":"Step 3: Provide Instructions for the Agent","text":"<p>Similarly to what we've previously done in the Model Playground, we'll now need to define the behavior of the agent, through the system prompt. </p> <p>Tip</p> <p>The Agent Builder provides a Generate feature that uses a large language model (LLM) to generate a set of instructions from a description of your agent's task. This feature is helpful if you need guidance in crafting the agent's instructions.</p> <p></p> <p>For the sake of this exercise, we'll leverage a set of instructions similar to the one used in the previous section:</p> <pre><code>You are an intelligent and friendly AI assistant that supports a social media management team creating content targeted to a developer audience, on different channels and formats.\n\n# Task\nYour role is to:\n- Engage with users in natural conversation to understand their social media content creation goals.\n- Ask thoughtful questions to gather relevant project details.\n- Be brief in your responses.\n\nContent Formats:\n- Blog: Informative or narrative articles, structured with an introduction, body, and conclusion.\n- Video Script: Text for videos, including directions for scenes, dialogues, and actions.\n- Post Captions: Short and catchy texts to accompany images or videos on social media.\n\n# Personality\nYour personality is:\n- Warm and welcoming, like a helpful colleague.\n- Professional and knowledgeable, like a seasoned social media expert.\n- Curious and conversational\u2014never assume, always clarify.\n\n# Steps\nThink step-by-step:\n1. Analyze the user's query to determine if it requests content about Microsoft technologies or tools.\n2. If YES:\n   - Plan what information is needed and select the appropriate MCP learn server tools to gather Microsoft documentation.\n   - Use \"mcp_learn_mcp_ser_microsoft_docs_search\" to identify relevant official Microsoft sources.\n   - If highly valuable/relevant pages emerge, follow up with \"mcp_learn_mcp_ser_microsoft_docs_fetch\" to retrieve the full content.\n   - Persist in using these tools until the answer is factually grounded and complete. Only terminate your turn when the problem is solved.\n   - DO NOT guess, invent, or proceed without tool results. Use your tools to learn any missing details.\n3. If NO:\n   - Proceed with general content creation and strategy for developer audiences, as specified by user.\n4. If the user asks for something outside social media content strategy, politely inform them you cannot assist outside of social media content creation.\n\n# Tool Use Guidelines\n- Always use MCP learn server tools (\"docs_search\", \"docs_fetch\") for Microsoft tech topics to ground your response.\n- Never guess or fabricate information about Microsoft technologies or tools.\n- If unsure about any details, always use the server tools to find out. Do not rely on assumptions.\n- Continue querying and planning with tools until your answer is fully supported and grounded.\n- Only finish your turn when the user\u2019s query is fully resolved and content is complete.\n\n# Guardrails\n- Stick to the scenario above. If something falls outside social media content creation, respond to the user politely with your scope limits.\n</code></pre> <p>Note how we added a couple of new sections to the instructions:</p> <ul> <li>A Steps section that guides the agent on how to approach user queries, including when and how to use the MCP tools.</li> <li>A Tool Use Guidelines section that provides specific rules for using the MCP tools effectively.</li> </ul> <p>In the next section, we'll cover more about the MCP server cited in the instructions and how to let our Social Media manager agent access its tools.</p>"},{"location":"04_Agent_Building/#step-4-add-tools-to-the-agent","title":"Step 4: Add Tools to the Agent","text":""},{"location":"04_Agent_Building/#start-the-microsoft-learn-mcp-server","title":"Start the Microsoft Learn MCP server","text":"<p>Earlier in the Model Augmentation exercise, we added grounding data to the model in the form of a .docx file attachment. While that may have been convenient for the sake of testing the base model prior to model selection, what we'd recommend is to ground the agent with data in such a way that's scalable to larger datasets and always up-to-date information.</p> <p>To achieve this, we can use the Model Context Protocol (MCP) server to provide the agent with access to relevant data sources. This allows the agent to retrieve up-to-date information and context as needed. The retrieval of relevant information is handled automatically by the MCP server, which communicates with the agent via the MCP standard, so that the agent can focus on generating responses based on the most relevant and current data.</p> <p>This project already includes the configuration to run the remote Microsoft Learn MCP server, which exposes search tools to query the Microsoft Learn documentation and search for official Microsoft/Azure code samples.</p> <p>To start the Microsoft Learn server, , within your Visual Studio Code workspace, go to Explorer from the side bar menu, navigate to <code>.vscode/mcp.json</code>. Within the <code>mcp.json</code> file, locate the <code>Learn MCP Server</code> and click Start above the server.</p> <p></p> <p>Note</p> <p>Once the server is started, you should see the status change to Running.</p>"},{"location":"04_Agent_Building/#add-search-tools-to-the-agent","title":"Add search tools to the agent","text":"<p>Once the server is running, return to Agent Builder. In the Tool section, click on the + button and then select MCP server.</p> <p></p> <p>Next, select Use tools added in Visual Studio Code.</p> <p></p> <p>Make sure you have the <code>mcp_learn_mcp_ser_microsoft_docs_search</code> and <code>mcp_learn_mcp_ser_microsoft_docs_code_search</code> tools selected, then click Ok.</p> <p></p>"},{"location":"04_Agent_Building/#step-5-test-tools-integration","title":"Step 5: Test tools integration","text":"<p>Now that the tools have been added to the agent, let's test them out. In the playground input area - where you see the placeholder <code>Type a message.</code>, enter the following prompt:</p> <pre><code>Create five short LinkedIn posts about the advantages of using AI tools from the Microsoft ecosystem for developer productivity.\n</code></pre> <p>In addition to the agent's response, you should see the details of the tool invocations within the chat area. Expand the details of the tool calls to see how the agent used the MCP tools to search for relevant Microsoft Learn documentation and code samples to ground its response.</p> <p></p>"},{"location":"04_Agent_Building/#key-takeaways","title":"Key Takeaways","text":"<p>In this section, you learned how to create an AI agent with Agent Builder in the AI Toolkit and equip it with tools via Model Context Protocol (MCP). You leveraged the Microsoft Learn MCP server to provide the agent with access to relevant data - the Microsoft official docs, allowing it to retrieve up-to-date information and context as needed.</p> <p>Click Next to proceed to the following section of the lab.</p>"},{"location":"05_Migrate_to_code/","title":"Migrate to Code","text":"<p>In this section, you will learn how to migrate the agent you've created in AI Toolkit to a code-based workflow.</p> <p>The AI Toolkit provides generated code for agents created in Agent Builder. You can choose your preferred SDK as well as programming language. Once you have your code file, you can integrate your agent into your own app.</p>"},{"location":"05_Migrate_to_code/#step-1-generate-the-code","title":"Step 1: Generate the Code","text":"<p>In Agent Builder, scroll down towards the bottom of the left side of the screen and select View Code.</p> <p></p> <p>When prompted, select your preferred client SDK (e.g. Microsoft Agent Framework) and programming language (e.g. Python). Once the new file is created, save the file to your workspace.</p>"},{"location":"05_Migrate_to_code/#step-2-view-the-code","title":"Step 2: View the Code","text":"<p>Before running the script, review the content of the file as there may be placeholders that must be modified before running. If you need assistance understanding the script logic, you could leverage GitHub Copilot Chat in Ask mode.</p> <p>Save the generated code file to your workspace as 'src/python/ai-agent.py'. Be sure to have the file active so that GitHub Copilot Chat can use the file as context. Alternatively, you could reference the specific file itself in your prompt to GitHub Copilot Chat.</p> <p>Note</p> <p>If you see a '+' icon besides the file name, that means ai-agent.py is suggested as context by GitHub Copilot Chat, but it's not yet added. Click on the '+' icon to add the file as context.</p> <p></p> <p>For example try the following prompt:</p> <pre><code>Explain what's happening in this script.\n</code></pre> <p>If there's any changes that need to be made, you could switch to Agent mode and request the changes to be made. You'll be requested to approve any file changes prior to committing the file updates to the script.</p>"},{"location":"05_Migrate_to_code/#optional-bonus","title":"(Optional) Bonus","text":"<p>If you'd like to run the code, save the file and follow the comments at the top of the code file. The instructions will vary as it's dependent on the client SDK and language selected.</p> <p>For example, if you selected the Microsoft Agent Framework SDK with Python, follow the instructions below:</p> <ol> <li>Locate the section in the code file that configures the MCP server tool. It should look similar to this:</li> </ol> <p><pre><code> def create_mcp_tools() -&gt; list[ToolProtocol]:\n return [\n     MCPStdioTool(\n         name=\"VSCode Tools\".replace(\"-\", \"_\"),\n         description=\"MCP server for VSCode Tools\",\n         command=\"INSERT_COMMAND_HERE\",\n         args=[\n             \"INSERT_ARGUMENTS_HERE\",\n         ]\n     ),\n ]\n</code></pre> 2. Replace the placeholders with the actual command and arguments, to configure the MCP server tool correctly. Your code snippet should now look like this:</p> <p><pre><code> def create_mcp_tools() -&gt; list[ToolProtocol]:\n return [\n     MCPStreamableHTTPTool(\n         name=\"Learn_MCP_Server\",\n         description=\"MCP server for Microsoft Learn documentation - provides search and fetch capabilities for official Microsoft docs\",\n         url=\"https://learn.microsoft.com/api/mcp\"\n     ),\n ]\n</code></pre> 4. Open a terminal in Visual Studio Code by selecting Terminal -&gt; New Terminal from the top menu. 5. Install the required dependencies by using:</p> <pre><code>pip install agent-framework --pre\n</code></pre> <ol> <li>Run the script using:</li> </ol> <pre><code>python ai-agent.py\n</code></pre>"},{"location":"05_Migrate_to_code/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Agent Builder automatically generates code for agents in multiple programming languages and SDKs, facilitating easy migration from prototype to production.</li> <li>Code files may contain placeholders that need modification before execution, requiring developers to understand and adapt the generated logic for their specific needs.</li> <li>Using GitHub Copilot Chat in Ask and Agent modes helps developers understand generated code and create additional components like UI elements for complete agent applications.</li> </ul> <p>Click Next to proceed to the following section of the lab.</p>"},{"location":"06_Bonus_Evaluations/","title":"Bonus: Manually Evaluate Your Agent Responses","text":"<p>Note</p> <p>This is a bonus section you can complete if you still have time during the allotted lab slot. Otherwise, you are more than welcome to go through it at your own pace once back home.</p> <p>In this section, you will learn how to manually evaluate a dataset of your agent's responses. Manual evaluations are when humans directly judge the quality of an LLM\u2019s output. In practice, this means a person reads the generated response and decides\u2014often against a rubric or simple scale\u2014whether it is correct, relevant, clear, or \u201cgood\u201d versus \u201cbad.\u201d With Agent Builder, you can complete manual evaluations to assess your agent\u2019s performance.</p>"},{"location":"06_Bonus_Evaluations/#step-1-add-data","title":"Step 1: Add Data","text":"<p>In Agent Builder, switch to the Evaluation tab. Executing an evaluation requires a value for the User Query field, which is the prompt that the user submits to the agent.</p> <p>You have a couple of options from here with respect to how you'd like to add data for your evaluation.</p> <p>Tip</p> <p>To expand the Evaluation section, click the Expand to Full Screen icon next to the Trash Can icon.</p> <p>Manually Add Data</p> <p>You can manually add your own data in the Evaluation tab, by creating an empty row and adding input for the User Query cell. </p> <p>Tip</p> <p>Use the Add an Empty Row button to create each row of the table and then double-click on a cell to edit its content.</p> <p></p> <p>Generate Data</p> <p>If you need help with creating data, the Generate Data feature can generate up to 10 rows of synthetic data. Synthetic data is artificially created data that mimics real-world information, but isn\u2019t collected from actual people or events. The feature itself leverages a LLM that takes Generation Logic as input to create User Query suggestions. The Generate Data feature generates its own set of instructions (or Generation Logic) based on the agent's Instructions. However, you can modify the Generation Logic to your liking.</p> <p></p> <p>After entering 5 as the number of Rows of Data to Generate, review the Generation Logic and select Generate to generate a dataset. The generated dataset appears in the evaluation table.</p> <p>Import a Dataset</p> <p>If you've created your own bulk dataset of User Query values, you could import the dataset to Agent Builder for evaluation. Agent Builder supports <code>.csv</code> files as input.</p> <p></p> <p>Consider experimenting with each option! The remaining instructions for this lab will continue to follow the second option: Generate Data</p>"},{"location":"06_Bonus_Evaluations/#step-3-assess-your-agent-output","title":"Step 3: Assess Your Agent Output","text":"<p>With your AI-generated dataset prepared, you can run rows one by one or select multiple rows to run together. To select all rows, check the box in the header row. To run the selected rows, select the Run Response icon (i.e. play button).</p> <p></p> <p>The model will generate a response for each User Query value. Once the response is generated, review the output and select either the thumbs up or thumbs down icon in the Manual column.</p> <p></p> <p>How do you decide whether the response deserves a thumbs up or thumbs down? When deciding whether to give a thumbs up or thumbs down, think about whether the output met your expectations. A thumbs up means the response was accurate, relevant, clear, and genuinely helpful\u2014it gave you the information or result you were looking for. A thumbs down means the response fell short in some way, such as being incorrect, incomplete, confusing, off-topic, or not useful for your task.</p> <p>In short, ask yourself: Did the output do what I needed it to? If yes, choose thumbs up; if not, choose thumbs down.</p>"},{"location":"06_Bonus_Evaluations/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Agent Builder supports manual data entry, synthetic data generation, and CSV imports, providing flexibility for creating evaluation datasets that match specific testing needs.</li> <li>Human judgment through thumbs up/down ratings helps assess whether agent responses meet expectations for accuracy, relevance, and usefulness beyond automated metrics.</li> </ul>"},{"location":"07_Summary/","title":"Summary","text":"<p>In this lab, you learned how to:</p> <ul> <li>Explore and compare models to select the best fit for your business scenario</li> <li>Augment models with prompts and data to get more accurate and grounded responses</li> <li>Prototype an agent by combining models and instructions with tools via MCP (Model Context Protocol)</li> <li>Extract the agent's code for further customization and deployment</li> </ul>"},{"location":"07_Summary/#next-steps","title":"Next steps","text":"<p>As you move forward in your development journey with AI agents, and consider deploying them in production environments, there's a few important considerations to keep in mind:</p> <ul> <li>Azure hosted models: While this lab focused on using GitHub-hosted models for prototyping, for production scenarios, it's advisable to use Azure-hosted models. These models offer better performance, reliability, and compliance with enterprise standards. You can explore the available catalog in Microsoft Foundry Models.</li> <li>Evaluation: Before deploying an agent, it's crucial to evaluate its performance thoroughly. This includes testing its responses for accuracy, relevance, and safety. Consider using a mix of automated tests and human evaluations. You can learn more about agent evaluation in the official documentation.</li> <li>Deployment: When deploying your agent, consider the infrastructure and platform that best suits your needs. An application similar to the one you prototyped in this lab - which includes a Python application based on Microsoft Agent Framework, a Microsoft Foundry hosted model and an MCP server - for example can be deployed using Azure Container Apps or Azure Kubernetes Service (AKS). These services provide scalability and reliability for production workloads.</li> <li>Monitoring: Once deployed, continuously monitor the agent's performance in real-world scenarios. This helps in identifying any issues or areas for improvement. Set up logging and alerting mechanisms to track the agent's behavior and performance metrics. The observability features in Microsoft Foundry can be very helpful for this purpose. Discover more in the official documentation.</li> <li>Continuous improvement: AI agents can always be improved. Gather user feedback and analyze the agent's interactions to identify areas for enhancement. Regularly update the agent's model, prompts, and tools to keep it effective and relevant.</li> </ul>"},{"location":"07_Summary/#useful-resources","title":"Useful Resources","text":"<ol> <li>Install the AI Toolkit in VS code: aka.ms/AIToolkit</li> <li>Read the AI toolkit docs: aka.ms/AIToolkit/docs</li> <li>Look at a real-world use case: aka.ms/BRK441GHrepo</li> <li>Share your learnings with the community and get support: aka.ms/foundrydevs</li> </ol>"}]}